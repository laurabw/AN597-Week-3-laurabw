---
title: "Module 8"
author: "Laura Brubaker-Wittman"
date: "9/24/2019"
output: html_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probabilities and Distributions

## Objectives

The objective of this module is to begin our discussion of statistical inference from a frequentist/classical
statistics approach. Doing so means that we need to cover basics of probability and distributions.

## Recap of Important Terms and Concepts

* **Population** - includes **all** of the elements from a set of data = ***N***
* **Sample** - one or more observations from a population = ***n***
* **Parameter** - a measurable characteristic of a *population*
* **Statistic** - a measurable characteristic of a *sample*

When we do **statistical inference** we are basically trying to draw conclusions about a *population* based on
measurements from a noisy *sample* or trying to evaluate whether it is reasonable to assume that our sample is drawn from a particular population.

This process of trying to draw conclusions is complicated by the fact that…

* Our sample may be *biased*, *non-random*, or *non-representantive* in some way
* There may be unknown or unobserved variables that impact how the sample is related to the population
* The assumptions we make about the population that our sample is drawn from might not be correct

***All of these are common in primatological data and should be carefully considered when performing analyses and
choosing statistical tests to run!***

## Probability

The term **probability** is applied to **population level** variables that describe the magnitude of chance
associated with particular observations or event. Probabilities summarize the relative frequencies of possible
outcomes.Probabilities are properties of **distributions**. Probabilities vary between zero and one. Outcomes that
are impossible have *Pr = 0*, those that are certain have *Pr = 1*.

Example: if we roll a (fair) die, there are 6 possible outcomes, each has a probability of occurring of 1 in 6.
This is referred to as a *frequentist* or *classical* way of thinking about the probability of different outcomes:
the relative frequency with which an event occurs over numerous identical, objective trials.

**Example: Rolling a Die**

We will use the {manipulate} package and the sample() function to explore the effects of sample size on estimates
of the probability of different outcomes. The probability of each outcome (rolling a “1”, “2”,…, “6”) is 1 in 6,
but our estimate of the probability of each possible outcome will change with sample size.

*Here, I ran the code chunk provided in the module, including installing and loading the {manipulate} package.
The code below not only runs the probability of rolling each side of the die randomly (so will be 
different each time you do it) over 100 trials, but also created a histogram of the results. The {manipulate}
package will also let you manipulate how many roles within R Studio without writing new code. A box pops up with a
slider function when you run the code in the console and you can adjust the "n" in real time. This is the part of
the code that reads (so you are saying you want to make a slider for an "n" from 100 to 1000, by increments of
100)*:

**n=slider(0, 10000, initial=100, step=100)**

```{r eval=FALSE, include=FALSE}
library(manipulate)
outcomes <- c(1,2,3,4,5,6)
manipulate(
    hist(sample(outcomes, n, replace = TRUE),
             breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5),
             probability = TRUE,
             main = paste("Histogram of Outcomes of ", n, " Die Rolls", sep=""),
             xlab = "roll",
             ylab = "probability"),
    n=slider(0, 10000, initial=100, step=100)
)
```

    library(manipulate)
    outcomes <- c(1,2,3,4,5,6)
    manipulate(
        hist(sample(outcomes, n, replace = TRUE),
             breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5),
             probability = TRUE,
             main = paste("Histogram of Outcomes of ", n, " Die Rolls", sep=""),
             xlab = "roll",
             ylab = "probability"),
        n=slider(0, 10000, initial=100, step=100)
    )
    
*Note: I was unable to run this code in R Markdown html rendering because manipulate() must be run in R Studio. I am assuming because of the slider code.*

## *CHALLENGES*

**CHALLENGE 1**

Write a function to simulate rolling a die where you pass the number of rolls as an argument. Then, use your
function to simulate rolling two dice 1000 times and take the sum of the rolls. Plot a histogram of those
results.


```{r}
nrolls <- 1000
roll <- function(x) {sample(1:6, x, replace = TRUE)}
two_dice <- roll(nrolls)+roll(nrolls)
hist(two_dice,breaks=c(1.5:12.5), probability = TRUE, main="Rolling Two Dice", xlab = "sum of rolls", ylab = "probability")
```

# Rules of Probability

1. *Pr*(+) = Probability that something occurs = 1

2. *Pr*(∅) = Probability that nothing occurs = 0

3. *Pr*(***A***) = Probability that a particular event ***A*** occurs

    0 ≤ *Pr*(***A***) ≤ 1 *somewhere between 0 and 1, meaning 0, as described means the event does not happen,
    while 1 means it happens all the time, and a decimal between 0 and 1 would represent the precentage of how
    often it happens*
    
4. *Pr*(***A***⋃***B***) = Probability that a particular event ***A*** -*OR*- a particular event ***B*** occurs
UNION (*which is what the U like symbol stands for*)

    **Formula**: *Pr*(***A***⋃***B***) = *Pr*(***A***) + *Pr*(***B***) - *Pr*(***A***⋂***B***)

    If event A and B are *mutually exclusive*, then this simplifies to *Pr*(***A***) + *Pr*(***B***) (*just
    the addition of the probability of **A** happening to the probability of **B** happening since mutually
    exclusive means they cannot happen at the same time*)
    
5. *Pr*(***A***⋂***B***) = Probability that both ***A*** and ***B*** occur *simultaneously* = INTERSECTION
(*which is what the upside down U like symbol represents*)

    **Formulas**: *Pr*(***A***⋂***B***) = *Pr*(***A*** | ***B***) × *Pr*(***B***) = *Pr*(***B*** | ***A***) × *Pr*(***A***)

    where the pipe operator ( | ) can be read as “given”.

    If the 2 events are *independent* (i.e., if the probability of one *does not* depend on the probability of
    the other), then *Pr*(***A***⋂***B***) simplifies to:

    *Pr*(***A***) × *Pr*(***B***)

    If *Pr*(***A***⋂***B***) = 0, then we say the events are *mutually exclusive* (e.g., you cannot have a die
    roll be 1 **AND** 2 simultaneously)
    
6. *Pr*(***Ā***) = Probability of the complement of A (i.e., not A) 

    **Formula**: 1 - *Pr*(***A***) (which makes sense if we think of the idea that *Pr*(***A***) will always be
    between 0 and 1 with 0 being it not happening and 1 being it happening always. So, if we think of 1 as the
    full chance of something happening, and we subtract the actual percentage of the time it *does* happen
    [expressed as a decimal], we are left with the time it *does not* happen, which is the *complement*) 
    
7. *Conditional Probability * is the probability of an event occuring after taking into account the occurrence of another event, i.e., one event is *conditioned* on the occurrence of a different event.

    For example, the probability of a die coming up as a “1” given that we know the die came up as an odd number
    (“1”, “3”, or “5”). *So we are looking at the probability of it being a 1 out of the three odd numbers we
    know it to be.*

    **Formula:** *Pr*(***A*** | ***B***) = *Pr*(***A***⋂***B***) ÷ *Pr*(***B***)

    If event ***A*** and event ***B*** are *independent*, then:
    
    *Pr*(***A*** | ***B***) = [*Pr*(***A***) × *Pr*(***B***)] ÷ *Pr*(***B***) = *Pr*(***A***)

    If event ***A*** and ***B*** are *dependent*, then:
    
    *Pr*(***A*** | ***B***) ≠ *Pr*(***A***)
    
**CHALLENGE 2**

You have a deck of 52 cards, Ace to 10 + 3 face cards in each suit. You draw a card at random.

* What is the probability that you draw a *face card*?

    *Pr*(***face card***) = 12 (*number of total face cards*) / 52 (*number of total cards*) = .231 (23.1%)
    
* What is the probability that you draw a *King*?

    *Pr*(***King***) = 4 (*number of total King cards*) / 52 (*number of total cards*) = .077 (7.7%)

* What is the probability that you draw a *spade*?

    *Pr*(***spade***) = 13 (*number of spade cards*) / 52 (*number of total cards*) = .25 (25%)
    
* What is the probability that you draw a *spade* ***given*** that you draw a *face card*?

    *Pr*(***spade*** | ***face card***) = *Pr*(***spade*** ⋂ ***face card***) ÷ *Pr*(***face card***)
    
    *Pr*(***spade***) = .25 (see above)
    
    *Pr*(***face card***) = .231 (see above)
    
    Therefore:
    
    *Pr*(***spade*** ⋂ ***face card***) = .25 x .231 (see **Probability Rules**) = .058
    
    
* What is the probability that you draw a King given that you draw a face card?

    *Pr*(***King*** | ***face card***) = *Pr*(***King*** ⋂ ***face card***) ÷ *Pr*(***face card***)
    
    *Pr*(***King***) = .077 (see above)
    
    *Pr*(***face card***) = .231 (see above)
    
    Therefore:
    
    *Pr*(***King*** ⋂ ***face card***) = .077 x .231 (see **Probability Rules**) = .018
    

    
* What is the probability that you draw a card that is **both** from a *red suit* (hearts or diamonds) **and** a
*face card*?

    *Pr*(***A***) = *red suit* = 26/52 = 1/2

    *Pr*(***B***) = *face card* = 12/52 =

    *Pr*(***A*** | ***B***) = *red suit* given *face card* = 6/12

    *Pr*(***A***⋂***B***) = *Pr*(***A*** | ***B***) × *Pr*(***B***) = 6/12 × 12/52 = 6/52 = 0.1153846
    
* What is the probability that you draw a card that is **either** a *club* **or not** a *face card*?

    *Pr*(***A***) = *club* = 13/52 = 13/52

    *Pr*(***B***) = *not a face card* = 40/52 (*which is the complement to probability of being a face card*)

    *Pr*(***A***⋂***B***) = *club* and *not a face card* = 10/52

    *Pr*(***A***⋃***B***) = *Pr*(***A***) + *Pr*(***B***) - *Pr*(***A***⋂***B***) = 13/52 + 40/52 - 10/52 = 43/52
    
# Random Variables

A **random variable** is a variable whose outcomes are assumed to arise by chance or according to some random or
stochastic mechanism. The chances of observing a specific interval has associated with it a **probability**.

Random variables come in **two** varieties:

1. *Discrete Random Variables* are random variables that can assume only a countable number of discrete
possibilies (e.g., counts of outcomes in a particular category). We can assign a probability to each possible
outcome. For example, with a die, each outcome of rolling a 1 -6 would be an equal 1/6 chance, while if a die had
the same number on multiple sides, the probability of rolling any given number may be differ.

2. *Contious Random Variables* are random variables that can assume any real number value within a given range
(e.g., measurements). We cannot assign a specific probability to each possible outcome value as the set of
possible outcomes is infinite, but we can assign probabilities to *intervals* of outcome values.

With these basics in mind, we can define a few more terms:

A **probability function** is a matehmatical function that describes the chance associated with a random variable
having a particular outcome of falling with a given range of outcome values.

We can also distinguish between *two* types of **probability functions**:

1. *Probability Mass Functions (PMFs)* are associated with **discrete random variables.** These functions
describe the probability that a random variable takes a particular discrete value.

To be a valid *PMF*, a function ***f(x)*** must satisfy the following:

1. There are ***k*** distinct outcomes ***x***~1~, ***x***~2~, ... ***x***~*k*~

2. **0** ≤ *Pr*(***X*** = ***x***~1~) ≤  **1** for all ***x***~1~

3. ∑ *Pr*(***X*** = ***x***~1~) for all ***x*** from ***x***~1~ to ***x***~*k*~ = **1**

*Example: Flipping a Fair Coin*

```{r}
outcomes <- c("heads","tails")
prob <- c(1/2,1/2)
barplot(prob,ylim=c(0,0.6),names.arg=outcomes,space=0.1, xlab="outcome",ylab="Pr(X = outcome)",main="Probability Mass Function")
```

```{r}
cumprob <- cumsum(prob)
barplot(cumprob,names.arg=outcomes,space=0.1,xlab="outcome",ylab="Cumulative Pr(X)",main = "Cumulative Probability")
```

*Example: Rolling a Fair Die*

```{r}
outcomes <- c(1,2,3,4,5,6)
prob <- c(1/6,1/6,1/6,1/6,1/6,1/6)
barplot(prob,ylim=c(0,0.5),names.arg=outcomes,space=0.1,xlab="outcome",ylab="Pr(X = outcome)",main="Probability Mass Function")
```

```{r}
cumprob <- cumsum(prob)
barplot(cumprob,names.arg=outcomes,space=0.1,xlab="outcome",ylab="Cumulative Pr(X)",main = "Cumulative Probability")
```

(Back to the two kinds of probabulity functions, the first being *PMFs*, the second being:)

2. *Probability Density Functions (PDFs)* are associated with continuous random variables. These functions
describe the probability that a random variable falls within a given range of outcome values. The probability
associated with that range equals the area under the density function for that range.

To be a valid *PDF*, a function ***f(x)*** must satisfy the following:

1. ***f(x)*** ≥  **0** for all −∞ ≤ ***x*** ≤ +∞. That is, the function ***f(x)*** is non-negative everywhere.

2. ∫^+∞^~−∞~ ***f(x)*** d***x*** = **1**. That is, the total area under the function ***f(x)*** = **1**

*An Example*

The **Beta Distribution** refers to a family of continuous probability distributions defined over the interval
[0, 1] and parametrized by two positive shape parameters, denoted by α and β, that appear as exponents of the
random variable ***x*** and control the shape of the distribution.
    
**Formula**: ***f(x)*** = ***K*** ***x***^α−1^ (**1**−***x***)^β−1^ 

If we set **k** = 2, α = 2, and β = 1 and restrict the domain of ***x*** to [0, 1] it gices us a triangular
function that we can graph as follows:

```{r}
library(ggplot2)
a <- 2
b <- 1
K <- 2
x <- seq(from= 0, to=1, by = 0.025)
fx <- K*x^(a-1)*(1-x)^(b-1)
lower_x <- seq(from=-0.25, to=0, by= 0.025) # add some values of x less than zero
upper_x <- seq(from=1, to=1.25, by= 0.025) # add some values of x greater than one
lower_fx <- rep(0,11) # add fx=0 values to x<0
upper_fx <- rep(0,11) # add fx=0 values to x>1
x <- c(lower_x,x,upper_x) # paste xs together
fx <- c(lower_fx,fx,upper_fx) # paste fxs together
d <- as.data.frame(cbind(x,fx))
p <- ggplot(data = d, aes(x=x, y=fx)) + xlab("x") + ylab("f(x)") + geom_line()
p
```

Is this a **PDF**? Why or why not?

*Yes!* It satisfies **both** criteria for a *PDF* (see above):

1. ***f(x)*** ≥  **0** for all −∞ ≤ ***x*** ≤ +∞.

2. ∫^+∞^~−∞~ ***f(x)*** d***x*** = **1**

The **cumulative distribution function**, or **CDF**, of a random variable is defined as the probability of
observing a random variable ***X*** taking the value of ***x*** or less, i.e., ***F***(***x***) = *Pr*(***X*** ≤
***x***).

```{r}
x <- seq(from=0, to=1, by=0.005)
prob <- 0.5 * x * K*x^(a-1)*(1-x)^(b-1)
barplot(prob,names.arg=x,space=0,main = "Cumulative Probability", xlab="x",ylab="Pr(X ≤ x)")
```

**IMPORTANT!** The built-in ***R*** function for the **Beta Distribution** [pbeta()], can give us the cumulative
probability directly, if we specify values of α = 2 and β = 1.

```{r}
pbeta(0.75, 2, 1) # cumulative probability for x ≤ 0.75
```

We can define the **survival function** for a random variable ***X*** as **S**(***x***) = *Pr*(***X*** > ***x***) = 1 - *Pr*(***X*** ≤ ***x***) = 1 - ***f***(***x***)

Finally, we can define the “qth” *quantile* of a *cumulative distibution function* as the value of ***x*** at which the *CDF* has the value “q”, i.e., ***F***(***x***~q~) = ***q***.

# Expected Mean and Variance of Random Variables

The **mean value** (or expectation) and the **expected variance** for a random variable with a given *probability mass function* can be expressed generally as follows:

μ~***X***~ = Expectation for ***X*** = ∑ ***x***~i~ × *Pr*(***X*** = ***x***~i~) for all ***x*** from ***x***~i~
to ***x***~k~

σ^2^~***X***~  = Variance of ***X*** = ∑ (***x***~i~ − μ~***X***~)^2^ × *Pr*(***X*** = ***x***~i~) for all
***x*** from ***x***~i~ to ***x***~k~

Applying these formulas to die rolls, we could calculate the expectation for ***X*** for a large set of die
rolls:

(1 * 1/6) + (2 * 1/6) + … + (6 * 1/6) = 3.5

```{r}
m <-sum(seq(1:6) * 1/6)
m
```

And the expected variance:

[(1 - 3.5)^2 * (1/6)] + [(2 - 3.5)^2 * (1/6)] + … + [(6 - 3.5)^2 * (1/6)] =

```{r}
var <- sum((seq(1:6)-mean(seq(1:6)))^2 * (1/6))
var
```

Likewise, we can calculate the expectation and variance for a random variable ***X*** with a given *probability
density function* generally as follows:

μ~***X***~ = Expectation for ***X*** = ∫^+∞^~−∞~ ***x*** ***f***(***x***) d***x***

σ^2^~***X***~ = Variance of ***X*** = ∫^+∞^~−∞~ (***x***−μ~***X***~)^2^ ***f***(***x***) d***x***

To demonstrate these numerically would require a bit of calculus, i.e., *integration*.